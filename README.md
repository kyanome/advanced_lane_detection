# advanced_lane_detection

### ◆ Camera Calibration

#### 1. Briefly state how you computed the camera matrix and distortion coefficients. Provide an example of a distortion corrected calibration image.

The code for this step is contained in the first code cell of the IPython notebook located in "Advanced_Lane_Finding_Project.ipynb".  

I start by preparing "object points", which will be the (x, y, z) coordinates of the chessboard corners in the world. Here I am assuming the chessboard is fixed on the (x, y) plane at z=0, such that the object points are the same for each calibration image.  Thus, `objp` is just a replicated array of coordinates, and `objpoints` will be appended with a copy of it every time I successfully detect all chessboard corners in a test image.  `imgpoints` will be appended with the (x, y) pixel position of each of the corners in the image plane with each successful chessboard detection.  

I then used the output `objpoints` and `imgpoints` to compute the camera calibration and distortion coefficients using the `cv2.calibrateCamera()` function.  I applied this distortion correction to the test image using the `cv2.undistort()` function and obtained this result:

![alt text][image1]

### ◆ Pipeline (single images)

#### 1. Provide an example of a distortion-corrected image.

To demonstrate this step, I will describe how I apply the distortion correction to one of the test images.I create the `undistort_image()` function that reuse the distortion coefficients (objpoints and imgpoints) given by `cv2.calibrateCamera()` to calibrate the test images.


Here's the output:
![alt text][image2]


#### 2. Describe how (and identify where in your code) you used color transforms, gradients or other methods to create a thresholded binary image.  Provide an example of a binary image result.

I used a combination of color and gradient thresholds to generate a binary image.  (thresholding steps  in `Advanced_Lane_Finding_Project.ipynb`).  

Here's the output:

![alt text][image4]    ![alt text][image3]


#### 3. Describe how (and identify where in your code) you performed a perspective transform and provide an example of a transformed image.

 I choosed an image of straight lines and selected points (Source) on these lines. In the "birds-eye view" the distance between these lines have to be the same at the bottom and at the top of the image, because we are looking at straight lines. I choosed destination points for cv2.getPerspectiveTransform() where this is true.

```python
src = np.float32([[(width/2) - size_top, height*0.65],
                  [(width/2) + size_top, height*0.65],
                  [(width/2) + size_bottom, height-50],
                  [(width/2) - size_bottom, height-50]])

dst = np.float32([[(width/2) - output_size, (height/2) - output_size],
                  [(width/2) + output_size, (height/2) - output_size],
                  [(width/2) + output_size, (height/2) + output_size],
                  [(width/2) - output_size, (height/2) + output_size]])
```

This resulted in the following source and destination points:

| Source        | Destination   |
|:-------------:|:-------------:|
| 585, 468      | 280, 0        |
| 710, 468      | 1000, 0      |
| 1010, 670     | 1000, 720      |
| 270, 670      | 280, 720        |

Here's the output:

![alt text][image5]

#### 4. Describe how (and identify where in your code) you identified lane-line pixels and fit their positions with a polynomial?

First, I computes a histogram of the bottom half of the image and finds the bottom-most x position of the left and right lane lines. Second, I changed these to quarters of the histogram just left and right of the midpoint. After that, I used a polynomial to fit their positions. After developing function for `fit_polynomials()`, I was able to detect the lanes and yield the following results:

![alt text][image6]

#### 5. Describe how (and identify where in your code) you calculated the radius of curvature of the lane and the position of the vehicle with respect to center.

On the step 4 a polynomial was calculated on the meters space to be used here to calculate the curvature. The Code is the following:

```python
left_curverad = ((1 + (2 * left_fit_cr[0] * y_eval * ym_per_pix + left_fit_cr[1])**2)** 1.5/ np.absolute(2 * left_fit_cr[0]))
right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])
```

The code used to calculate this could be found at In [15].


#### 6. Provide an example image of your result plotted back down onto the road such that the lane area is identified clearly.

The generated points where mapped back to the image space using the inverse transformation matrix generated by the perspective transformation. The code used for this operation could be found on In [14], and the following images are examples of this mapping:


I implemented this step in lines # through # in my code in `Advanced_Lane_Finding_Project.ipynb` in the function `render_lane()`.  Here is an example of my result on a test image:

![alt text][image7]

---

### ◆ Pipeline (video)

#### 1. Provide a link to your final video output.  Your pipeline should perform reasonably well on the entire project video (wobbly lines are ok but no catastrophic failures that would cause the car to drive off the road!).

Here's a [link to my video result](project_video_output.mp4)

---

### ◆ Discussion

#### 1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?

I decided to go for the suggestions from class and use white and yellow filter in in HLS colorspace, and Sobel X filter on grayscale. This turned out to be very hard to tune correctly.To solve this problem. I used a combination of color and gradient thresholds to generate a binary image. But, I am yet to try all the techniques to be used for building a robust pipeline, so that I want to continue finding nice methods.





[//]: # (Image References)

[image1]: ./project_images/chess.png "Undistorted"
[image2]: ./project_images/undistort2.png
[image3]: ./project_images/combine.png
[image4]: ./project_images/combine2.png
[image5]: ./project_images/transform.png

[image6]: ./project_images/poly.png "Fit Visual"
[image7]: ./project_images/output.png "Output"
[video1]: ./project_video_output.mp4 "Video"
